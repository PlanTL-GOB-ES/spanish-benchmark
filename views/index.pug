extends layout

block title
  title The Spanish Evaluation Benchmark

block description
  meta(name='description', content='The Spaish Evaluation Benchmark (EvalES) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.')

block extralinks
  link(rel='stylesheet', href='./stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin squad_2_model_display(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th EM
      th F1
      th Extra Col 1
      th Extra Col 2
      th Extra Col 3
    - var human_em = 86.831
    - var human_f1 = 89.452
    - var largest_em = Math.max.apply(null, group.map(function (model) { return model.em; }))
    - var largest_f1 = Math.max.apply(null, group.map(function (model) { return model.f1; }))
      tr.human-row
        td
        td
          | Human Performance
          p.institution Stanford University
          a(href="http://arxiv.org/abs/1606.05250") (Rajpurkar & Jia et al. '18)
        td #{human_em}
        td #{human_f1}
        td
        td
        td
    each model, index in group
      if index < 4
          tr
            if is_test
              td 
                p #{model.rank}
                span.date.label.label-default #{moment.unix(model.date).format('MMM DD, YYYY')}
            td(style="word-break:break-word;")
              if model.submit_link
                a.link(href=model.submit_link) #{model.model_name}
              else
                | #{model.model_name}
              p.institution #{model.institution}
              if model.link
                a.link(href=model.link) #{model.link}
            td
              if model.em == largest_em
                b #{model.em.toPrecision(5)}
              else
                | #{model.em.toPrecision(5)}
            td
              if model.f1 == largest_f1
                b #{model.f1.toPrecision(5)}
              else
                | #{model.f1.toPrecision(5)}
            td
            td
            td

block content
  .cover#contentCover
    .container
      .row
        .col-md-15
          .infoCard
            .infoBody
              .infoHeadline
                h2 Leaderboard
              p.spaced SQuAD2.0 tests the ability of a system to not only answer reading comprehension questions, but also abstain when presented with a question that cannot be answered based on the provided paragraph.
              +squad_2_model_display(test2, true)
        .col-md-15
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is EvalES?
              p 
                span
                  | Eval
                  b ES 
                | is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or 
                i span
                | , from the corresponding reading passage, or the question might be unanswerable.
              hr
              p
                | Eval
                b  ES 
                |  combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.
              a.btn.actionBtn(href="/explore/v2.0/dev/") Explore SQuAD2.0 and model predictions
              a.btn.actionBtn(href="http://arxiv.org/abs/1806.03822") SQuAD2.0 paper (Rajpurkar & Jia et al. '18)
              h2 Have Questions?
              p 
                | Ask us questions at    
                a(href="mailto:plantl-gob-es@bsc.es") plantl-gob-es@bsc.es
                | .
            .infoSubheadline
              include includes/tweet
              include includes/github
