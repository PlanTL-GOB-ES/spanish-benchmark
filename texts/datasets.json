{
    "Text classification": "The Multilingual Document Classification Corpus (MLDoc) (Schwenk and Li, 2018; Lewis et al., 2004) is a cross-lingual document classification dataset covering 8 languages. We used the Spanish portion to evaluate our models on monolingual classification. It consists of 14,458 news articles from Reuters classified in four categories: Corporate/Industrial, Economics, Government/Social and Markets.",
    "Named Entity Recognition and Classification (NERC)": "We selected the CoNLL-NERC and the CAPITEL-NERC datasets. CoNLL-NERC is the Spanish dataset of the CoNLL-2002 Shared Task (Tjong Kim Sang, 2002). The dataset is annotated with four types of named entities: persons, locations, organizations, and other miscellaneous entities. They are formatted in the standard Beginning-Inside-Outside (BIO) format. The dataset is composed of 8,324 sentences with 19,400 named entities for the training set, 1,916 sen- tences with 4,568 named entities for the development set, and 1,518 sentences with 3,644 named entities for the test set. CAPITEL-NERC was the first sub-task of the CAPITEL-EVAL shared task, held by IberLEF in 2020. The source of the CAPITEL-NERC datasets is the CAPITEL corpus12 (Porta-Zamorano and Espinosa-Anke, 2020), a collection of Spanish articles in the news domain. The dataset consists of 22,647 sentences with 31,311 named entities for train, and 7,550 sentences for development and test sets respectively, with 10,229named entities for the development set and 10,226 for the test set. CAPITEL-NERC is annotated with the same four named entities used in CoNLL-NERC (persons, locations, organizations, and other), but following a Beginning-Inside-Outside-Ending-Single (BIOES) format.",
    "Paraphrase Identification": "The Crosslingual Adversarial Dataset for Paraphrase Identification (PAWS-X) (Yang et al., 2019) is a multilingual dataset that contains 49,401 training sentences, 2,000 sentences for the development set, and another 2,000 for the test set. It is important to note that this dataset contains machine translated text, and as a consequence some of the Spanish sentences might not be entirely correct.",
    "Part-of-Speech Tagging (POS)": "We selected the Universal Dependencies Part-of-Speech (UD-POS) dataset, from the Spanish Ancora corpus13 (Taulé, Martí, and Recasens, 2008), and the CAPITEL-POS from the CAPITEL Corpus, described above.",
    "Semantic Textual Similarity (Agirre et al., 2012)": "We collected the Spanish test sets from 2014 (Agirre et al., 2014) and 2015 (Agirre et al., 2015). Since no training data was provided for the Spanish subtask, we randomly sampled both datasets into 1,321 sentences for the train set, 78 sentences for the development set, and 156 sentences for the test set. To make the task harder for the models, we purposely made the development set smaller than the test set.",
    "Textual Entailment": "We used the Spanish part of the Cross-Lingual NLI Corpus (XNLI) (Conneau et al., 2018). This evaluation corpus consists of a collection 400,202 sentences, annotated with textual entailment via crowdsourcing.",
    "Question Answering (QA)": "We built a new dataset, the Spanish Question Answering Corpus (SQAC), an extractive QA dataset that we exhaustively present in section 3.2.1. There is no sizable training dataset analogous to the English version of SQUAD (Rajpurkar et al., 2016), and most finetunings of Spanish models rely on machine translated text. There is a professionally translated version of the XQUAD (Artetxe, Ruder, and Yogatama, 2019) dataset, but it is not bigenough or varied enough to properly train or evaluate, and the source text is not written originally in Spanish (and translation artifacts could slip in)."
}
